what is microservice ?


all logic to one feature work correctly.



challenge : Data management between services

each service gets its own database.
services never will reach to each other dbs.


it is pattern : Database per service

we want run it independetly.





Sync and Async in microsevices :


Sync : services communicate with each other using direct request
       web of dependency



Async : Services communicate with each other with events. event based
        Event bus , each service emit events or receive events from event
        bus.


second way async communicate :
each service , one db.

how catch our need info from other serivces.

simontinously save to db and send event to Event bus. for any body care 
about it.




============================================================



Blog Post application :



============================================================


running service with Docker :


build with docker files and docker ignore files.

each service.


now we want use k8s

service: sth that give us access to running ads or running
container.

kubectl apply -f posts.yaml 

kind pod : it is object wrap up container.
metadata: config ooptions for object we are going to create.
spec : list of config options the pod that we are about to create. 


k get pods



deployment is a k8s object that is intended to manage set of pods.
 it check pods exists.
 use for update.


#      selector        says he k8s look at all pods with created with matchlabels param and watch it

in template labels define label for selector.


// many thing not saved like 70 lines



skaffold , 

==============================================


the big challenge is managing data :




async , 


STUBHUB


===========================================================


k8s cluster = nodes + matster for manage

node = a vm run our containers

pod = almost a running container technically

deployment = monintors set of pods, make sure they are.

service =  provide a easy remmebmer url to access other running container


---

k8s config files :

it say to k8s about objects (deployments , pods , services)

in yaml syntax

it is kind of documentation for cluster
allow others know what are you doing

create pod

create image 
define file
create with kubectl
kubectl apply -f nameoffile


about config :


apiVersion : api of object we want use it must contain set of objects

kind : type of object from api

metadata : config we want to apply to the object,
        options we want apply like name

spec : exact attributes we want our oject have, list of config options  

        containers : we can create many containers in a pod
                we add array of them here


run yaml file with  kubectl 

first build image\
push to hub
write yaml file for pod/deployment and service
use kubectl to build pod/deployment and service

kubectl apply -f posts.yaml
kubectl get pods

docker exec -it ... = k exec -it ...

k logs ... = show log of terminal for microservice


deployment is managar for pods
write yaml file for it.

spec : how deployment must behave

in spec section of deployment
selector says he k8s look at all pods with created with matchlabels param and 
watch it for me

when use deployment if we delete pod or any problem , deployment will 
generate new one

k describe deployment 
k logs [pod name]
check event






for update deployment :
there are ways for this .
1 : update image in hub. update image version in yaml.
    now must inform k8s. k apply -f file.name -> show configured.
    (not good to change that number each time)
2 : use latest tag on image or leave it off. update code . build image .
    push to hub . then use command
    k rollout restart deployment [depl name]






now services :
by services we allow to use our pods (before it is hidden)
service is object. default type is ClusterIp.
4 type ClusterIp (expose pods only in cluster) and 
       loadBalancer(outside w) for daily use
       nodeport (dev outside w) 
       externalName (redirect to c name)





in spec/selector of serivce we put name of pod we want expose to world

port listen on LVM (also allow access out world).
targetPort listen on our piece of app.


nodePort(browser) -> port(NodePort service) -> targetPort(pod)

[minikubei ip]:[nodePort]/${route}





write services in one file
hint : ClusterIp services exists with NodePort or loadBalancer at same.

hint hint : use k explain pods or services so cool




how communicate between serivces???
change name of route in axios or what ever to name of
http://[ClusterIp service name]:[port(other ClusterIp)]/[route you defined]
(this case is when you have not famous message broker)




loadBalancer service :

cra go in pod.
in spa case all req come to pods from browser.
therefore we must only serve static files.

but how cra connect to others

cra --> loadBalancer service ->> it connect to all ClusterIp s

now make sure cra connect to it 
and make yaml for loadBalancer





loadBalancer service : tells k8s to reach out to its provider and provision a 
        loadBalancer . get traffic to single pod
        (do it in front with nginx)
        cluster is in cloud provider we want out world traffic in.
        make config for it . feed to cluster 
        it says to our cluster go and reach out to our cloud provider
        and provision sth called LoadBalancer
        it exists outside cluster in cloud provider - part of google or aws


Ingress or Ingress Controller : a pod with a set of routing rules to distribute traffic
        to other service.
        (do it with ingress-srv)
        it work along side top service and give it rule of routing to choose best
        pod 



outside(react browser)-> loadBalancer(cloud)-> cluster(ingress(rule routes)-> srv.pods)





we use ingress-nginx (check github) 
ingress-nginx = create both loadBalancer  +  Ingress

also there is another kubernetes-ingress

go website -> deployment -> installitaion guide


run mandatory command (give u yaml)
for this add  ingress from his website or addons enable minikube

provider specific command (give u yaml)
for this define yaml
{
        kind: Ingress
}

annotations : help ingress Controller understand we try to feed him some rules 
so ingress continusly scan objects and try to find someone who has below annotaions
kubernetes.io/ingress.class: nginx

nginx have some rule for me
allow us use regex

spec:
        rules : teach ingress here
        define host : [posts.com]
               http
                 paths
                   path
                   backend
                     serviceName
                     serivcePort




with k8s and his cluster we can host many sites 
we trick our machine to conect local host go to posts.com
edit -> /etc/hosts add [minikube ip] [name of host you want]



Now CRA :
change rotue to posts.com
make yaml file for react

last step :
add rules to ingress service
ingress can not do routing base on Method of request










To update and change env in dev env we use automate tool
https://stackshare.io/skaffold/alternatives
can use helm or other in prod




skaffold :
automate task in k8s
you can not use in prod

can change set of objects quickly.


create yaml file for it in root of all proj

when stop it delete all objects.
local/push : false prevent it to push and build image to docker hub
on each save

artifacts:  array of each image with their config
        if change is not in manual path , skaffold rebuild image
        like changes in package.json



to start
skaffold dev 


must always use to nodemon to make changes.

sometimes detecting not change we fix it in future.


----------------------------

NExt Project :

big challenge is data ,
we use async communication (with message brokers) with events
with async every service is self sufficient 100%
we use k8s for scale and deploy

we build npm to share code - central lib for share services
must define all our events in our library to know what happen
we also handle concurrency


overview of app :

STUBHUB

user collection , tickets collection ,  order collection , charge


we have 5 services

auth, tickets , orders , expiration , payments


events and architecture :
userCreated, userUpdated ... OrderCancelled , OrederCreated , ...
ticketCreated , chargeCreated ...


client(nextjs)-->
common (shared lib) -->
microservices with mongo , redis -> 
nats streaming server

----

auth service start 

install dependencies

tsc --init 

create src

index.ts

simple server 

now start k8s infrastructure from start

make docker file
make docker ignore file

build image
docker build -t penkong/stubhub-auth .

make infra/k8s folder 

wrtie auth-deployment with ClusterIp service

make skaffold - it read from local
(deploy section load all yaml file we want
in sync manual dest: . means where to sync file to inside running container
get file from src and through it container
)

start minikube

skaffold dev (if not reload use this in package ts-node-dev --poll src/index.ts)


now we want access our service
now set up ingress-nginx (loadBalancer --> ingress Controller)


make it available in minikube or server


write yaml for nginx - define domain
browsers does not trust ssl from server 
type anywhere in tab thisisunsafe

-------------------------

SECTION GO TO GOOGLE CLOUD :
need card
two way 

1 . by add sync section in skaffold.yaml new config 

2 . now when we change unsynced file --> skaffold got google cloud build 
    --> upload image to google cloud vm

now go through setup

cloud.google.com/free --> go sign up

create project -> go to project -> kluster engine -> cluster
create cluster -> setting 

kubectl context to connect to google cloud
kubectl use context as connection settings

to add context 2 way in gcloud dashobard or use google cloud sdk

sdk -> cloud.google.com/sdk/docs/quickstarts

install 

terminal  -> gcloud

gcloud auth login

init -> gcloud init

choose 

setup context now 

with docker running 
gcloud container clusters get-credentials <cluster name = ticketing dev>


now update skaffold config

enable gcloud build -> tools -> cloud build
update yaml -> after config -> change image name in deployment also
setup ingress-nginx on gcloud cluster -> command from website 
update hosts file to point there -> get loadBalancer ip from google ->
        network service -> get ip of loadBalancer -> add to /etc/hosts
restart skaffold


---------------------------------------------------

Section 7 : response normalization response

let go for auth proj

define router 
use them 
use express-validator to validate and sanitazation body of req
        add array az middleware validator with body()

import type of req, res
make route complete

but we need same error response for all of error back from all services
need similar structure for error

we make middleware for error, use it in app , throw error from router ,
refine error on middleware 

we define subclass of error for any reason.

created costum handler and use it by middleware

give type to class and error message return 

finally we used abstract class to make error to use

-----------------------------------

Section 8 : database modeling 

install mongoose 

each service has own db

express app => mongoose <=> mongoDb instance with deployment and clusterIP

with skaffold crete it

lifecycle of pods : if we delete pod of mongo db we lose 
how solve ? we back here 

we register mongoose with start() function style

now model and signup flow 
questions : 

req
1 . does user already exists?
2 . do we hash password when user enter?
3 . create new user and save to db
4 . send back jwt token for user logged in
res

why mongoose and ts have problem with each other???
because name of collection and docuemnt is same .
ts don't understand of prop pass to new User({here})
docuemnt make change must teach ts about it



create user model

to be sure of info pass to model , 
creat interface in model

use custom function for build model with ts
like this userSchema.statics.build

but ts does not understand how attach func to object

there fore need make new interface that desc the prop user model have

new interface extends mongoose.Model and we attach build() to it


now must fix second proble of when model create is different from 
what back from mongo

we do new interface that describe info come back from model 

and put it as first type(generic) in model and also response of  build()

generic : functions for types, args for function model


now signup process

check with findOne user exist
define new error for it

hash pass step
use for signup and signin
hash password -> produce unique string for that pass
we do hash stuff in model but because keep things clean
we do login in seprate class and only consume it there.
make services folder -> for general purpose things
when we want save user a lifecycle(middleware) 
from monog call presave and
do hashstuff
and in that middleware you must call done()

we use scrypt from crypto (it is hashing function)
use promisify with scrypt
scryptAsync bring us back a buffer

create user
save
return user (jwt in future)


------------------------------------------------

Section 9 : authentication 

this is so challenging part

there are many approaches 

main question : is this person logged in???

option 1 : 
sync requetst to auth serivce (centralize auth service)
but if auth service go down ???  - down side 



options 1.1 : 
every request go through single gateway that would authenticate
the incoming request -> then go to relate service
but if auth service go down ??? - down side


options 2 :
teach each individual serivce how to authenticate a user
each contain logic to inspect jwt/cookie and decide 
no dependency to out service 
duplicate logic of auth - down side  -> use share library
more down side : if other serivce make user invalid how other know?


we go with option 2 because we want independet services


let's do it :

make cookie for 15 minutes

if token is old( 30 min ) -> from logic inside service -> goto auth service
--> it give us refresh token 

or after (15 min) we say to user go to auth service and get new token

if we want ban user -> send event to ban user 



reminder cookie and jwt :
header 
        Set-Cookie : value  -> auto store to browser 
        browser send it with 
        request in 
header 
        Cookie : value


payload --> jwt algorithm --> jwt

we can put jwt on -> request header with Authorization 
                             body token
                             header Cookie


cookies are transport mechanism , move any kind of data , auto manage
        by browers

JWT auth/authorization mechanism , store any data we want , 
        we have to manage it manually 


our system need after authentication , send us info about user accesses
or buy power or ... , roles , accesses , expiration , diff language


on request we verify user authentication 

we use nextjs - ssr 
nextjs go to get info to generate ssr info therefore all auth 
do in server 
we need to know auth info from first reques because next do things
on server base on info came from server 

therefore we can not intercept request to attach Authorization to header
or there is not any body request




browser must send -------    jwt on Headers with Cookie
(service worker can work here)


we use cookie-session because let us store info in cookie by itself     


we want be sure content of cookie understand by differenet languages
and cookie-session use crypto 
what we do?
we not encrypt cookie

add cookie-session and types 

use it with options 

change setting on express itself

app.set('trust proxy' , true);

because traffic is proxied to our app through ingress-nginx
and express i dont like it . express let it happen


now everything ok , we must generate jwt and set it to cookie 
consider cookie-session do all other things

we store some info inside cookie by 
req.session
it is object created by cookie session 


npm i jsonwebtoken

by jwt.sign() we create > jwt
arg 1 : payload -> info we want store in jwt
arg 2 : signkey -> our secret


by jwe.verify() we become sure nobody change jwt
also use draw out payload


after save user to db we generate jwt with sign
hint : must save secret to k8s guys


then set it on req.session 

it save on base64 encoded
first decode base on 64
and after that you will see jwt

it have payload inside it , 


now sign key : 
now when we recive that jwt inside another service we need jwt sign
key -> but it bad because other can change -> but we must exchange it
it profession way

storing secrets with k8s

it have feature for it




we create new type of object inside cluster
object secret
when we create it we can use it inside node / pods as env var



create secret
kubectl create secret generic jwt-secret --from-literal=jwt={'hree'}
                          name of secretobj      assign key=value
generic is kind of secret , there are kinds for it
also secret like access image in docker repo


k get secrets


now must store in inside pod as env -> deployment

in containers -> add env / name : name of var as it show inside container
JWT_KEY

kubectl create secret generic jwt-secret --from-literal=JWT_KEY=123456asdf
       -name: JWT_KEY
        valueFrom:
          secretKeyRef:
            name: jwt-secret
            key: JWT_KEY



now want access it from code :
process.env.JWT_KEY!


to keep gaurd exist of env we must do it on init
on start 
if (!process.env.JWT_KEY) throw new Error('JWT_KEY MUST BE DEFINE')





now we must common response props 
like errors :

we do it in model with projection like 

on schema / second arg / toJSON : { transform(doc, ret) {}}
check code



NOW SIGN IN ROUTE :
steps like sign up at first


draw out validation stuff and make it middleware
base on args on middleware express understand type of middleware
when it is 4 it get it is error middleware

we now dont capture error (like error handler middleware) 
we produce error here

this validateRequest use over almsot every put and post request

check exist of user
if exist
compare password
if ok
generate jwt
and ...


current user :
react can not look at cookie
can not be access 
therefore we must somehow provide

if no cookie -> send current user null

check valid cookie token

if exist -> send current user from verify cookie
jwt.verify()
because it throw error we must use trycatch


we will add authorization stuff as middleware in near future


signout : 
req.session = null



now we need currentUser middleware :
when request come to other services
we want to know user is logged in or not or exist

and we add currentUser to request 
that let that service consume info of user from req
and dont need use jwt to decode 
decode and verify in middleware
register info on req -> service consume it

hint :
!req.session || !req.session.jwt == !req.session?.jwt


to add prop to exist object (req.currentUser)

we want augment it 
// to add new prop to it
declare global {
  namespace Express {
    interface Request {
      currentUser?: IUserPayload
    }
  }
}



second middleware reject if not user.
only check req.currentUser if not extist throw error



-------------------------------------------------

Section 10 : testing isolated microservice
what our test scope???

test a single code in isole like middleware(unit test) 

or

test how different piece of code work like req flow throuh middleware and
handler 

or 

test how differenet component work like request to service ensure write
to database or eventbus  

or

test how differenet service work like create a payment that have impact
on multi microservice ----> not now - not here


we do test microservice in isolation


test goal :

1. basic request handling -> test whole microservice *****

2. some test around model -> unit test like -> like test model

3. event emitting and receiveing -> send and receive evetns ->
it is like simulating service works together.


HOW WE EXEC TEST AT ALL ???
for now with npm run  but we go through more complex




TESTING ARCHITECTURE :

we implemet testing goal 1 

jest - test runner lib

steps:
start in momory copy of mongo
start express
use supertest lib to make fake request --> need some refactor
  we must have access to express app 
  we need to it , 
  but there are a lot of stuff there
  and also maybe port is in use by another (jest run concurrently)
  supertest if it not listen -> goto ephemeral port (random available)
  therefore we must not listen on 3000
  and split index to 2 file
  app.js -> express app not listen on any port
  export to test(ephemeral) and also to index.js(3000)
run assertion to make sure things right



do refacotr ot app.ts and index.ts


also we use mongodb-memory-server 
it allow us to run multiple mongo at same time

it is huge to prevent from dl on docker image add to 
npm install --only=prod


now add script 
test": "jest --watchAll --no-cache",
no-cache because of typescrpt

add "jest" field to packge.json

setup files after env , by this we tell jest run setup files after we initially 
start our test

add setup related stuff for mongo and other stuff


create mongo memory server allow us run differenet suites test at same time
let us direct access to 
get uri
pass to mongoose 

now before each test we reach to mongo and delete all data related

after all test done we stop server and close connections


NOW TEST :

for each folder in your service make __test__ folder for let jest know

signup .test.ts

use
supertest to make fake request to exprss app
request use chain syntax

npm run test

but we got error because in our test we dont have access to env vars

add to setup   process.env.JWT_KEY = '123456asdf'

ts maybe have problem with jest just restart it.

not test with wrong input

hit enter to run test

can write 2 seprate requetst insdie test with await request
does not matter you must either do await or return request


unique test email

inspect header has cookie set 
res of signup has get method that have access to header
but test failed 
because cookieSession share cookie if request come from https

but supertest do http
when jest run make NODE_ENV to test
secure: process.env.NODE_ENV !== 'test'

we can continue many test


signin test suites :
normal test


signout also test suite 

currentUser test suite 
test env like browser and postman have not ability to send cookie with request
therefore currentUser is null , how fix it???


two way easy and refacotr way

get cookie from signup attach it to currentUser
get it 
include next request with chain .set('Cookie', authResponse)
res.body.currentUser.email

but because it is repetetive
we use signup as helper function


write it in setup.ts as global function attached to all test


test for model and event handling will come in future.


--------------------------------------------------------


SECTION 11 : intergrate react ssr with nextjs

we build auth section of react app in this field


we do stuff on server

browser requets to => nextjs server => goto take data => 

i react react-dom next`

dev : next
 
add files you need 

add docker file

add kuberfile you need

add ingress config

add skaffold config


add next.config.js to handle update page on change 
next auto read this file 
we say poll all data in project every 300ms  

we use bootstrap
import from dist 

create _app

destructure Component and pageProps 
what are these??

on next when you navigate to distinct page , next import from page files
then next wrap it up in own his custom component 
refered as app
we define app and law each page will have

Component is page component 
pageProps are set of components we are intended to pass to each page 

items we want in each page will be here

npm i bootstrap


signup form :
in pages
build folder auth
add signup.js page in it
make form 

form submit -> k8s (nginx -> ingress -> clusterip -> microservice)

after submit go to devtools / network / xhr / signup > see cookie


add error handling and ... 

make custom hook for request


state of page in ssr :
browser -> server(nextjs -> currentUser route ) if ok -> render html with appropraite
info



challenge : how make request while application is being built???
steps :

next inspect url and ready set of comps

call those with ```getInitialPros``` (called on server) static -> return of it go 
as props on landing page

render each comp with those

produce html and send it back









fetch data during ssr :
hooks like useRequest work inside components 

because axios with getInitialProps go to server and inside there call 
axios auto attach info of url as base url and inside server there is no url 
that is nginx world
node http layer attach address of machine to request 


solution : 
consider follow up request must have cookie inside getInitialPros
config axios baseURL

browser -> nginx -> nextjs pod -> nginx -> auth pod

sync request to clusterIP work on same namespace 
pods are on default namespace
but 
ingress-nginx has own namespace



namespace is like sandbox - to orignize different objects
k get namespace



we must do cross namespace communication
http://nameofservice.namespace.svc.cluster.local

to get active services in namespace
k get services -n <namespace name>

in my system ingress live on kube-system namespace

http://ingress-nginx-controller-admission.kube-system.svc.cluster.local

now must pass cookie with it.

we use external namespace services

it map name of domain correctly



how we  find out request is on browser or server???

if we navigate from one page to another page after init load 
getInitialProps will call from client side

typeof window === 'udefined'
use top link 
otherwise use normal

kubectl get svc --all-namespaces
kubectl get pods --all-namespaces 



we have error with kube-system set 


when we have not access to browser on getInitialProps , we have ctx object
it have req , we send req with request to let us have cookie 
with it 

cookie is on req.heasers


through improvemnt

we must  extract logic

the logic of typeof window , we will do there with
axios.create({config})


devtools > application > cookies > domain cookie


make sign in page like sign up


reuse header :
we centralize get currentuser in _app 

but we have challenge

but _app is not page in each
page -> context == {req, res}
_app -> context == AppTree Component rotuer ctx:{req,res}
Object.keys(appContext)

when we use getInitialProps on _app no more getInitialProps on other pages
will call (not invoke on rest pages )
how fix it???

in appContext > Component > getInitialProps we have access to getInitialProps
of current page we on it

then we invoke it and make sure pass context to it
appContext.Component.getInitialProps(appContext.ctx)

now maybe on some pages we dont define getInitialProps therefore we must
handle undefined case with if check

now return { pageProps and data we fetched on _app }


do header and .. 

signout :

make a page 


-----------------------------------------------------------------



SECTION 12 : CODE sharing


ticket service : crud ticket

need auth to create or edit ticket

need middleware to check auth , 

we share error system , auth middleware and request validation middleware

---

now How build share libray???

there are method for sharing :
1. copy paste
2. use git submodule = add a git repo to current one
        auth repo , ticket repo , common repo
3. publish npm package __>>> we do this


---

package security : 

can publish as private 
or 
public registery > inside our orignization

public orignization and private orignization


create orignization 

make common 
make package correct
make git for it
commit 
npm login
npm publish --access public


we need type definition 
write as ts -> publish as js

we use some dependencies to help us

tsc --init
i del-cli typescrpt

make src/index.ts
write code 
add script 
config tsconfig

un comment 
declration : it make sure when we build it make type definition

now build

use del-cli to clean last code

make main correct in package
also add 
"types": "./build/index.d.ts",
to package json

add files 
it say to npm what files we want to publish

add .gitignore

commit 

npm version patch --> update minor version
npm run build 
npm publish


automate these process (not real world maybe)



move error and middleware in common


export * from source 

add dependencies

pub it 

use it in auth


for republish 

after that 
npm update @baneeem/common


to check version on pod

k exec -it [pod name] sh
goto node_modules
cd @your package 
cd common
cat pacakge.json

-----------------------------------------------------

SECTION 13 : crud ticket server

copy past test app and index from auth 
add dependencies 
build image 
make ticket deployment
make scaffold 
make ingoress


---

we need fix our way of connection to mongo 
in each microservice

add uri as env to yaml file 
and consume it in index.ts of tickets or wherever
in real world make it secret



--- 

now crud test for tickets 

go tdd for each routes



now need fake authentication for other service test

fabricate a cookie
with steps in code 
// build a jwt payload . { id, email }
// create jwt with key in test env 
// build session  { jwt: 'our jwt decoded from base 64' }
// session to json
// encode json
// return a cookie set


remmebmer : supertest want cookie return in an array


write tests with set('cookie', global.fnc())


after that go for createTicketRouter creation


express validator check set error on request if exist 

now for test that we want to know is sth written to db or not???

need to make model ,
reminder , interfaces , doc describe props a saved record has
and model describe all props a model has (like attached function)

ret : is object that return to json .


after ticket model creation go to test for make test with creation on db.

then write logic for route 

next show with id tests

and write logic 

but we have kind of error that not provided by customError 
change code in node_modules to handle that

aha id must cast ot _id , therefore it is bad request not not found.

but we must handle unknown error -> need udate common error

now generate correct type of id for test get :id
const id = new mongoose.Types.ObjectId().toHexString()

write other test routine check code to understand


---------------------------------------------------------------

SECTION 14 : NATS streaming server

to share events 
docs.nats.io 

nats and nats streaming server are different.
nats ==> nats streaming server 

it have his own design decisions that heavily affect us.

use official docker > read doc

command line are important to set .


make nats - deployment

add command line options in spec > containers section to yaml of pod 

args: [args to provide to primary command that exec when pod run]
args: ['-key', 'value', 'key', 'value']

it service also has difference
in ports and client and monitoring ports also

run it with skaffold





overview of nats :

before we used express and axios for events 
now we use a client library named node-nats-streaming


we do standalone project to understand mechanics of NATS


service(nats-event-streaming) ---> server ---> service(nats-event)


nats s requires us to subscribe to channels. events emitted to specific
channels / topics

publish to that channel and if anybody listen to this will get it.



events store in memory by default also we can add mysql or pgsql 
 





sub projcet :

npm init 
install dependencies
make publisher
make listener 
make scripts

implementation :
by use of  nats create client(or stan) to send events 




now we want connect publisher program to our cluster
publisher <> ingress-nginx <> nats cluster ip <> nats pod

or 

publisher <> nodeport <> nats pod

or 

set port forward inside kluster to go to pods
k get pods 
get nats pod name 
copy 
k port-forward [name of nats pod] <local port>:<nats port on cluster>
npm run publish
we are connected 


---

subject is name of channel we want share data there

also in listener we inform it from subject he can listen to 
we listen on subscription


nats.connect('ticketing', clientID : '123', {
  url: 'http://localhost:4222'
})

change data to json before send
stan.publish
stan.publish('ticket:created', data || message || event , () => {
  cb that let us know event published 
})


hint : console.clear()

on listener create subscription
listen on message


import Message interface to help us know inside msg of listener
getSequence : return number of events from number one 
getData : return data assosiceted to msg


we have on publihser and listener 
now consider we have more order microservice
we want scale up (vertically cpu ... , horizantally nodes)


now if we connect another listener we get error , clientId  registered
client id was 123 , and its already registered





server maintains list of clients and channels 
we use random name for client that listen to in code 
in k8s world handle it is easy




Queue Group :
now when we publish two pods react and consume and do stuff 
it bad because we do 2 time logic in for ex db or anywhere else

we dont want every event to in every copy of a service


we use queue group inside a channel
we can have multiple queue group associated with one channel
we give it name 
now every instance of order service create subscription to that queue and joins
now event come nats look at all queue we have and then randomly send event to one
of the services inside on queue

the service that not registered to any group recieve all 



how set up?
on listener when create subscription second arg 
 

ack mode :
we define options for listener subscription
it is chain of methods
and 
put it as 3th arg in subscription

now maybe in process in one of subscription we lose db or server down or data invalid
and event lose
this is default behavior
but we want to process it second time if for any reason it fucked up
setManualAckMode(true)
now after complete we ack message
if not after 30 it send it again
msg.ack()



client health check :

now if we reload listener and in mean while send event 
that does not appear after after 30 s re broadcast

monintor port 8222
expose thsese port on local
k port-forward nats-deployment-86bb8f68d4-nm5wf 8222:8222
http://localhost:8222/streaming

http://localhost:8222/streaming/clientsz
it show all clients publisher and listener

http://localhost:8222/streaming/channelsz?subs=1
it show more info about channelsz

nats wait to service back for 30 s and after that send message to other up service
but it not good idea for us
we do 2 things

1. in args of deployment
hbi , is heartbeat - how often health check 
hbt , how long each client has to respond
hbf , number of fails can each client can have before nats consider it failed

2. gracefully shutdown 
by code 
we inform nats from problem and inform him dont send message
stan.on('close', () => {
        console.log('nats closed')
        process.exit()
})

process.on('SIGINT', () => stan.close())
process.on('SIGTERM', () => stan.close())
SIGINT : interupt signals
SIGTERM : terminate signals
on windows it not work or force kill process



with all of these it can cause events come outside of order












Core concurrency issue :
not ordered message go to different microservices

if any event fail it is catastrophic event for business


solution bad 1 : run copy of each service -> but not work also cause process bottleneck
solution bad 2 : write code for handle it.
...


solution :

this is system design > should redesign service
use transactional database to record and after that dispatch events

that version ing pattern help us to prevent this
mongo has versioning stuff built in .




event redelivery :
what about losted event when a service is off or whatever
we have event history 
but deliver all is not correct way because maybe we are in scale process

we use a durable subscribe option
.setDurableName('oreder-service')

nats have list off all durable subscription we have
when it ack()
nats store it 
and when that come up it send unprocessed events.

if we dont use queue gropup on subscription on server down nats remove durable 
history .


--------------------------------------------------------------

SECTION 15 : nats with nodejs 


move it to common module

it is abstract class 
we are going to create class call listener with props and fields below 
subject , onMessage , client , queue gropup name, ackwait , subscriptionOptions 
listen , parsemessage .




we join queue group with other services



on sample project still ,

make abstract class out of litener logic and let us all other service extend it 
it finally become an npm moduele

refactor code .





now use typescrpt and generic class to make our inherited class 
of base listener impelemnt a specific type of interface


subject enum 
make business related interface coupled with enum


now goto base listener and define Event interface 

and use it to make listener a generic class


make publisher base class to same steps

make it accept async / await 



now how test those ????
listener need handle it.


alternatives to define type in common module is JSON schema , protobuf
apache avro

now we must draw out some code for common npm 

build and publish that npm 


restart nats 



--------------------------------------------------------------------


SECTION 16 : manage a nats client 

make class out of commond module for publish 


create nats client in our project

it must be a singleton because we want share client with other
part of projects.

remember gracefully shutdown is so important

private _client? : Stan => ? means it may stay undefined for some time 


promisify it connect and after that 

now time to use it inside routes
but because of probable undefined case 
we use getter  


now must be sure destroy nats on shutdown or whatever else

put that logic in index because we dont want it in common with other files and services


we have 3 main class for nats publisher listener and wrapper 



failed event publishing :
we can await but it bring latency 
and do we must await for success publish message???

if connection lose we have false data integrity 



handling publish failed :
we save event to database and save published state 
and seprate code watch events

we must do transactional with transaction and event.

we dont do it but we must read it and learn.





test with nats :

we use fancy feature of jest .
fake use mock 
redirect import 

totally way for case like this 
1. find the files you want fake like natsWrapper
2. in same directory make folder called __mocks__
3. create identical file in folder like original one 
4. write fake implementation
5. tell jest use that fake 
jest.mock('relative path to file  to original file ')
jest automatically redirect that to __mock__


fake implementation:
just define things from parent class that have usage in your test
client and publish

add mock func to global file setup 


now embed publish test in tests

for that inside __mock make mock functtion  jest.fn().mockImplementation()

then write test and use main natsWrapper but jest provide us mock one

between each single test need reset mock function
jest.clearAllMocks()




now nats Env Variable :
clusterId , clientId and server address
add those to ticket deployment 

we want traceable client id 
we use name of pod for it 
how??
- name: NATS_CLIENT_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
add those to index



remember we must save all events in database transactionally


--------------------------------------------------------------------------

SECTION 17 : cross service data replication 



order service 
it must listen to 2 event ticket:created and ticket:updated

duplicate code 
i npm 
build image 
create k8s dep
skaffold
ingress

---

like ticketing 
make routes

---

create order :
check body ticketId validate it with mongoose like 
.custom((input: string) => mongoose.Types.ObjectId.isValid(input))

we want get ticketId and base on that create order record and save it 

create order model
we want reference ticketId or embed docuemnt

how find out ticket reserved or not???
we use ref/population feature


in define interface of new model with ref style 
we add field in natural (ticket not ticketId) and define type 
for it ITicketDoc

in make schema we add ref: 'name of model'  to that field .



add to common lib orderStatus Enum 
and use it in interface and also in schema by below syntax
enum : Object.values(OrderStatusEnum)


mongoose ref : 'Ticket'  
there are 3 way to assocaiate order and ticket together 
this section . vid 11

now create ticket model 

and use ITicketDoc as interface in order definition to use as ref in schema





order creation logic :

we make comments in code for show different case s 

also when we have common question - query from our model we embed it inside 
model itself as field then we can call it on model instance to check condition 


when we want add method to model use => schema . statics
when we want add method to docuemnt use => schema . methods (always function(){})


write logic without nats 


go for test 




fetch user Order :
all case
with populate

fetch one order 
show case
test

delete canecel order :
update status to cancelled
test


--------------------------------------------------------------


SECTION 18 : understand event flow 

create new events on common npm for order created and cancelled

share time stamp with string 


publisher for new and delete 


after that make test ready for added publisher 



---------------------------------------------------------------

SECTION 19 : listener and handling concurrency issues

we put listener for ticket and order



make listener for ticket created 

after successfully process we must do ack()

we want do data replication on orders with ticket info

but for handle concurrency we must add versioning . 

ticket service -> order service
  ticket               ticket

must assign id of first ticket as second ticket id challenge 
must adjust build func  add id to interface
also must mongo accept it as _id


remember : different listeners on a service have common queue group name 

Now Listener for UpdatedTicket


now must create instance of each and start to listen on it .


after nats started we listen to listener in index.ts


handling concurrency :
need data versioning 
mongo do this for us .

google for startegy Optimistic concurrency Control 

on change mongoose update version field of doc automatically


how impelemnt these in mongoose ????

we use module in mongoose 
first step 
handle ticket service
mongoose-update-if-current npmjs
attach it to schema 
scheme.plugin()
also we say to mongoose we want to rename default name of version
ticketSchema.set('versionKey', 'version')

also must help ts to understand doc from now on has version key

write test for it 

when using trycatch pattern in jest test you must inform it from finish test 
help jest out
by an arg to func done()






now where update and who update verion :

add version to event in common 

add version to publisher in services


update model with version in other service order 

and update code in onMessage on listeners with below criteria :
first check version is one below 
if true 
update in table 

test with script file .




now better we abstract query method 
in mongoose

build helper method on ticket model or else 
to handel fetch version in abstract 


behind scene of mongoose update if current 


how reach database order ?
k get pods 
get mongo related service
k exec -it podname mongo 
open shell


write test for listeners 


---

update orders model with version 
fix tests 
fix route orders with publish version 

---

liteners on ticket service

lock ticket if order created instantiated 
with what strategy lock ticket ???
save orderId on ticket model on ticket service 


now test for order created listener 


because of version ing system we must inform others from change in order created 
had impact on version of ticket  


make client of Listener base class protected that let all other listeners use
client inside and dont be tied to other publisher 


write test for order created listener 

check mock function args



make OrderCancelled listener on ticket to inform remove id of order id from ticket
doc in collection 


dont forget to listen on index in ticket service


update , update route in ticket to dont let process for pre ordered ticket

------------------------------------------------------------------------

Section 20 : Expiration worker serverics 


scheduled message not support by nats

we use bulljs 

we save msg in redis as jobs and after 15 min expire happen . 

remove all server related stuff .

make docker image , k8s 
for redis and service

add config to expire deployment in env section 
to how to connect to redis

this service not directly connect to any part of app then 
we remove port section in deployment file 

there fore we dont need any clusterIP service at all

always add file sync to scaffold file .


steps 
make listener for created order 
make bulljs 
make publisher 

with bull we make queue and queue processing 

make queue folder
make expiration queue file 
it that enque a job 
on time 
job out of redis 
expiration queue process job




we only put orderId in queue

making queue 
first arg name of channel 
second arg option object > which instance we want connect 
now define interface that describe info to stick in job


write job processor inside the queue file 
info sits on job.data




now add to queue a job on order event .
use add on queue 
first item payload 



add listeners to index




delay job second arg in add 


add event complete to common moduel


make publisher for expire complete 

in order listen to it  and do logic

write test for it 


to access all different times an function invoked in test
(natsWrapper.client.publish as jest.Mock).mock.calls
it is array


---------------------------------------------------------------

Section 21 : handling Payment 


start with normal flow of creaion of a micro service 

input           order creatd and cancelled 
collection      cahrges , orders 
output          charge creatd


accross services we need replicate id s remember that .


make models for order in payment




















































